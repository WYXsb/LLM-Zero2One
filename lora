import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class LoRALayer(nn.Module):
  def __init__(self,in_features,out_features,merge=False,rank=8,lora_alpha=16,dropout=0.1):
    super().__init__()
    self.in_features = in_features
    self.out_features = out_features
    self.merge = merge
    self.rank = rank

    self.linear = nn.Linear(in_features,out_features):
    if rank > 0:
      # nn.parameter把矩阵变成可训练的参数
      self.lora_a = nn.Parameter(torch.zeros(out_features,rank))
      nn.init.kaiming_normal_(self.lora_a,a=0.01)
      self.lora_b = nn.Parameter(torch.ones(rank,in_features))
	  self.scale = lora_alapha / rank
	  # 原始权重需要设置为不可以训练
	  self.linear.weight.requires_grad = False
	  self.linear.bias.requires_grad = False
	self.dropout = nn.dropout(dropout) if dropout > 0 else nn.Identity()
	if merge:
	  self.merge_weight()
def merge_weight(self):
	if self.merge and self.rank > 0:
		self.linear.weight.data += self.scale * (self.lora_a @ self.lora_b)

def unmerge_weight(self):
	if self.rank > 0:
			self.linear.weight.data -= self.scale *(self.lora_a @ self.lora_b)
